{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BD_OPERACOES em linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sidrapy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(f'{SALVA_OS_DFS}\\\\{nome}.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lumm\\\\OneDrive - Elera\\\\Documentos\\\\Python Scripts\\\\book_trading_python\\\\Dataframe\\\\w.xlsx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nome = 'w'\n",
    "f'{SALVA_OS_DFS}\\\\{nome}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abre os dataframes\n",
    "bd_operacoes = pd.read_pickle('bd_operacoes.pickle')\n",
    "curva_forward = pd.read_pickle('curva_forward.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "from workalendar.america import Brazil\n",
    "from datetime import date, datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from ehubAPI import forwardCurve\n",
    "\n",
    "#Logins e Senhas\n",
    "EHUB_EMAIL = \"guilherme.cruz@elera.com\"\n",
    "EHUB_SENHA = \"Ber@2021!\"\n",
    "\n",
    "#Diretórios\n",
    "FORECAST_ASSUMPTIONS_DIR = r\"C:\\Users\\lumm\\OneDrive - Elera\\Documentos\\Python Scripts\\Book no BI\\Recursos\\Forecast Assumptions.xlsx\"\n",
    "BD_OPERACOES_DIR = r\"C:\\Users\\lumm\\OneDrive - Elera\\Documentos\\Python Scripts\\Book no BI\\Recursos\\BD_OPERACOES.xlsm\"\n",
    "INFLACOES_DFS_DIR = r'Z:\\Risk\\28. Portfolio Data Base\\09.Inflação\\Dataframe'\n",
    "SALVA_OS_DFS = f'{os.getcwd()}\\Dataframe'\n",
    "\n",
    "#Constantes\n",
    "MES_INICIAL = '2022-01-01'\n",
    "\n",
    "ANOS_BD_OPERACOES = 5\n",
    "MESES_BD_OPERACOES = ANOS_BD_OPERACOES * 12\n",
    "\n",
    "ANOS_OPERACIONAL_BOOK = 2\n",
    "MESES_OPERACIONAL_BOOK = ANOS_OPERACIONAL_BOOK * 12\n",
    "\n",
    "ANOS_TOTAL_BOOK = 5\n",
    "MESES_TOTAL_BOOK = ANOS_TOTAL_BOOK * 12\n",
    "\n",
    "COL_INICIO_VOLUME = 26\n",
    "COL_INICIO_PRECO = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _salva_dataframes(df, nome, caminho=None, formato='Excel'):\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(caminho):\n",
    "            os.makedirs(caminho)\n",
    "\n",
    "        if formato == 'Excel':\n",
    "            df.to_excel(f\"{caminho}/{nome}.xlsx\")\n",
    "\n",
    "        elif formato == 'csv':\n",
    "            df.to_csv(f\"{caminho}/{nome}.csv\")\n",
    "\n",
    "        else:\n",
    "            df.to_pickle(f\"{caminho}/{nome}.df\")\n",
    "        print(f'> O Dataframe \"{nome}\" foi salvo na pasta {caminho}.\\n')\n",
    "    \n",
    "    except:\n",
    "        print('> Não foi possivel salvar. Algum parametro esta errado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta um dataframe com todas os meses e horas dos meses \n",
    "def _periodo_e_horas_meses(mes_inicial=MES_INICIAL, qtd_meses=MESES_TOTAL_BOOK):\n",
    "       \n",
    "    try:\n",
    "        col_periodo = []\n",
    "        col_horas_mes = []\n",
    "        mes = mes_inicial\n",
    "\n",
    "        if isinstance(mes_inicial, str):\n",
    "           mes = datetime.strptime(mes, '%Y-%m-%d')\n",
    "\n",
    "        for i in range(qtd_meses):\n",
    "\n",
    "            horas = ((pd.to_datetime(mes) + pd.DateOffset(months=1)) - mes)*24\n",
    "            horas_int = int(str(horas)[:3])\n",
    "\n",
    "            col_horas_mes.append(horas_int)\n",
    "            col_periodo.append(mes)\n",
    "            \n",
    "            mes = pd.to_datetime(mes) + pd.DateOffset(months=1)\n",
    "\n",
    "        periodos_e_horas = pd.DataFrame({'Periodo': col_periodo, 'Horas': col_horas_mes})\n",
    "        print('> Dataframe de meses e horas montado com sucesso')\n",
    "        \n",
    "        #Salva o Dataframe\n",
    "        _salva_dataframes(df=periodos_e_horas, nome=\"periodos_e_horas\",formato='Excel')\n",
    "\n",
    "        # Retorna um Dataframe\n",
    "        return periodos_e_horas\n",
    "\n",
    "    except:\n",
    "        print('> Data precisa estar formatada em AAAA-MM-DD e quantidade de meses precisa ser um numero inteiro positivo')\n",
    "        sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta a BD Operacoes para o Book de Trading\n",
    "def captura_bd_operacoes(caminho_bd=BD_OPERACOES_DIR, col_inicio_vol=COL_INICIO_VOLUME, col_inicio_preco=COL_INICIO_PRECO,qtd_meses=MESES_BD_OPERACOES):\n",
    "\n",
    "    bd_operacoes_arq = pd.read_excel(caminho_bd, sheet_name=\"BD_COMERCIAL\", skiprows=3, usecols=\"B:FK\")\n",
    "    bd_operacoes_arq.dropna(subset=['COD_WBC'], inplace=True)\n",
    "\n",
    "    # Monta um Dataframe com os meses e as horas dos meses\n",
    "    df_periodos_e_horas = _periodo_e_horas_meses()\n",
    "\n",
    "    # Transformando os COD WBC em unicos \n",
    "    bd_operacoes_arq['Duplicados'] = bd_operacoes_arq['COD_WBC'].duplicated(keep=False)\n",
    "    cod_wbc_duplicados = 0\n",
    "    todos_cod_wbc = []\n",
    "\n",
    "    for i, linha in bd_operacoes_arq.iterrows():\n",
    "\n",
    "        if linha['Duplicados']:\n",
    "            todos_cod_wbc.append(f'wbc_{cod_wbc_duplicados}')\n",
    "            cod_wbc_duplicados += 1\n",
    "\n",
    "        else:\n",
    "            todos_cod_wbc.append(linha['COD_WBC'])\n",
    "        \n",
    "    bd_operacoes_arq['COD_WBC'] = todos_cod_wbc\n",
    "\n",
    "    # Monta o DF de Volumes da BD Operações,transpondo os volumes para linha\n",
    "    bd_operacoes_vol = bd_operacoes_arq.iloc[:, col_inicio_vol : col_inicio_vol + qtd_meses]\n",
    "    bd_operacoes_vol.columns = df_periodos_e_horas['Periodo'].to_list()\n",
    "    bd_operacoes_vol['COD_WBC'] = bd_operacoes_arq['COD_WBC']\n",
    "    bd_operacoes_vol = bd_operacoes_vol.melt(id_vars=[\"COD_WBC\"],var_name=\"Periodo\",value_name=\"Vol MWm\")\n",
    "    bd_operacoes_vol = bd_operacoes_vol.set_index(['COD_WBC','Periodo'])\n",
    "\n",
    "    # Monta o DF de Precos da BD Operações, transpondo os precos para linha\n",
    "    bd_operacoes_preco = bd_operacoes_arq.iloc[:, col_inicio_preco : col_inicio_preco + qtd_meses]\n",
    "    bd_operacoes_preco.columns = df_periodos_e_horas['Periodo'].to_list()\n",
    "    bd_operacoes_preco['COD_WBC'] = bd_operacoes_arq['COD_WBC']\n",
    "    bd_operacoes_preco = bd_operacoes_preco.melt(id_vars=[\"COD_WBC\"],var_name=\"Periodo\",value_name=\"Preco\")\n",
    "    bd_operacoes_preco = bd_operacoes_preco.set_index(['COD_WBC','Periodo'])\n",
    "\n",
    "    # Unifica os DFs de volume e preco\n",
    "    bd_operacoes_vol_e_preco = bd_operacoes_vol.join(bd_operacoes_preco,on=['COD_WBC','Periodo']).reset_index().set_index('COD_WBC')\n",
    "\n",
    "    # Monta o DF com as informações de cada contrato, sem volume e preco\n",
    "    bd_operacoes_sem_vol_preco = bd_operacoes_arq.iloc[:,:col_inicio_vol]\n",
    "    bd_operacoes_sem_vol_preco = bd_operacoes_sem_vol_preco.join(bd_operacoes_arq['Compra/venda']).join(bd_operacoes_arq['_'])\n",
    "    bd_operacoes_sem_vol_preco = bd_operacoes_sem_vol_preco.set_index('COD_WBC')\n",
    "\n",
    "    # Monta o DF final com todas as operações, as informacoes, volumes e precos\n",
    "    bd_operacoes = bd_operacoes_sem_vol_preco.join(bd_operacoes_vol_e_preco, on='COD_WBC').reset_index(drop=True)\n",
    "    bd_operacoes = bd_operacoes.dropna(subset=['Vol MWm', 'Preco']).set_index('COD_WBC').reset_index(drop=True)\n",
    "\n",
    "    #Salva o Dataframe\n",
    "    _salva_dataframes(df=bd_operacoes, nome=\"bd_operacoes\",formato='Excel')\n",
    "\n",
    "    # Retorna um Dataframe\n",
    "    return bd_operacoes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL BD OPERACOES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_bd_operacoes(df, book= 'Trading'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "(0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n852    False\n853    False\n854     True\n855     True\n856     True\nName: MOD PREÇO, Length: 857, dtype: bool, 'MOD PREÇO')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, key, axis, is_setter)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1212\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1213\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m             \u001b[0mkeyarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer_for\u001b[1;34m(self, target, **kwargs)\u001b[0m\n\u001b[0;32m   4960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4961\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4962\u001b[0m         \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3166\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3167\u001b[1;33m             return this.get_indexer(\n\u001b[0m\u001b[0;32m   3168\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_engine_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.lookup\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1784\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1785\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m   1786\u001b[0m             \u001b[1;34mf\"{repr(type(self).__name__)} objects are mutable, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Series' objects are mutable, thus they cannot be hashed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18816/1480531592.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Modificação do Preco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mlinhas_de_pld_mais\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_uniao_bd_e_precos_book\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MOD PREÇO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'PLD + Spread'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinhas_de_pld_mais\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'MOD PREÇO'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_uniao_bd_e_precos_book\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinhas_de_pld_mais\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Preco'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf_uniao_bd_e_precos_book\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlinhas_de_pld_mais\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Valor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Preco com IPCA = Ajustado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lumm\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;34m\"unhashable type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ensure_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexingError\u001b[0m: (0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n852    False\n853    False\n854     True\n855     True\n856     True\nName: MOD PREÇO, Length: 857, dtype: bool, 'MOD PREÇO')"
     ]
    }
   ],
   "source": [
    "df1 = bd_operacoes_modificada\n",
    "\n",
    "# Filtra para somente pegar um book\n",
    "df1 = df1[df1['BOOK'] == 'Trading']\n",
    "\n",
    "# Une os DFs da BD operacoes com o Periodo e horas e com o Precos do Book\n",
    "df_uniao_bd_e_periodo = df1.merge(periodos_e_horas, left_on='Periodo', right_on='Periodo', how='left')\n",
    "df_uniao_bd_e_precos_book = df1.merge(precos_book, left_on='Periodo', right_on='Periodo', how='left')\n",
    "\n",
    "# Modificação do Preco\n",
    "linhas_de_pld_mais = df_uniao_bd_e_precos_book['MOD PREÇO'] == 'PLD + Spread'\n",
    "df1.loc[linhas_de_pld_mais,'MOD PREÇO'] = df_uniao_bd_e_precos_book.loc[linhas_de_pld_mais,'Preco'] + df_uniao_bd_e_precos_book.loc[linhas_de_pld_mais,'Valor']\n",
    "\n",
    "# Preco com IPCA = Ajustado\n",
    "linhas_de_preco_ajustado =  ~pd.isnull(df_uniao_bd_e_precos_book['ÍNDICE'])\n",
    "df1.loc[linhas_de_preco_ajustado,'MOD PREÇO'] = 'Ajustado'\n",
    "\n",
    "\n",
    "# Preenche com as horas, Volume em MWh e o PLD (Preço de mercado)\n",
    "df1['Horas'] = df_uniao_bd_e_periodo['Horas']\n",
    "df1['Vol MWh'] = df1['Vol MWm'] * df_uniao_bd_e_periodo['Horas']\n",
    "df1['PLD'] = df_uniao_bd_e_precos_book['Valor']\n",
    "\n",
    "#Cria colunas vazias\n",
    "df1['Preco MTM'] = [pd.NaT] * len(df1)\n",
    "df1['CNPJ Contraparte'] = [pd.NaT] * len(df1)\n",
    "\n",
    "# PLD+\n",
    "linhas_de_pld_mais = df_uniao_bd_e_precos_book['MOD PREÇO'] == 'PLD + Spread'\n",
    "df1.loc[linhas_de_pld_mais,'MOD PREÇO'] = df_uniao_bd_e_precos_book.loc[linhas_de_pld_mais,'Preco'] + df_uniao_bd_e_precos_book.loc[linhas_de_pld_mais,'Valor']\n",
    "\n",
    "# Venda\n",
    "linhas_de_venda = df_uniao_bd_e_precos_book['Compra/venda'] == 'V'\n",
    "df1.loc[linhas_de_venda,'Preco MTM'] = df_uniao_bd_e_precos_book.loc[linhas_de_venda,'Preco'] - df_uniao_bd_e_precos_book.loc[linhas_de_venda,'Valor']\n",
    "df1.loc[linhas_de_venda,'CNPJ Contraparte'] = df_uniao_bd_e_precos_book.loc[linhas_de_venda,'CNPJ COMPRADOR'] \n",
    "\n",
    "# Compra\n",
    "linhas_de_compra = df_uniao_bd_e_precos_book['Compra/venda'] == 'C'\n",
    "df1.loc[linhas_de_compra,'Preco MTM'] = df_uniao_bd_e_precos_book.loc[linhas_de_compra,'Valor'] - df_uniao_bd_e_precos_book.loc[linhas_de_compra,'Preco']\n",
    "df1.loc[linhas_de_venda,'CNPJ Contraparte'] = df_uniao_bd_e_precos_book.loc[linhas_de_venda,'CNPJ VENDEDOR'] \n",
    "\n",
    "df1['Receita MTM'] = df1['Vol MWh'] * df1['Preco MTM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5344519.508928"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bd_operacoes\n",
    "filtro_1 = (df['SUBMERCADO'] == \"SE\")\n",
    "filtro_2 = (df['FONTE'] == \"Conv\")\n",
    "df.loc[filtro_1 & filtro_2, 'Vol. MWh'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13950.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bd_operacoes\n",
    "filtro_1 = (df['COD_WBC'] == 11691)\n",
    "filtro_2 = (df['Periodo'].dt.strftime(\"%Y-%m\") == \"2023-01\")\n",
    "df.loc[filtro_1 & filtro_2, 'Vol. MWh'].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast Assumpations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para decidir qual é o ultimo dia util antes da data enviada\n",
    "def _ultimo_dia_util(formato='data'):\n",
    "    calendario_br = Brazil()\n",
    "\n",
    "    try:\n",
    "        dia = datetime.today().date() - pd.DateOffset(days=1)\n",
    "\n",
    "        while calendario_br.is_working_day(dia) == False:\n",
    "            dia = pd.to_datetime(dia) - pd.DateOffset(days=1)\n",
    "        \n",
    "        if formato == 'str':\n",
    "            dia_util = dia.strftime('%Y-%m-%d')\n",
    "        elif formato == 'data':\n",
    "            dia_util = dia.date()\n",
    "            \n",
    "        print(f'> O ultimo dia util foi : {dia_util}')\n",
    "\n",
    "    except:\n",
    "        print(\"Formatação da data incorreta: 'AAAA-MM-DD'\")\n",
    "\n",
    "    # Retorna uma data\n",
    "    return dia_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta a curva forward do EHUB\n",
    "def ultima_curva_forward():\n",
    "\n",
    "    #Define o ultimo dia util antes de hoje\n",
    "    dia_util = _ultimo_dia_util('str')\n",
    "\n",
    "    # Dataframe da Curva Forward do EHUB \n",
    "    curva_forward = pd.DataFrame(forwardCurve(dia_util, EHUB_EMAIL, EHUB_SENHA))\n",
    "\n",
    "    #ETL da Curva Forward do EHUB\n",
    "    periodo = []\n",
    "    for linha in curva_forward['vertexDate']:\n",
    "        data = date(year= pd.to_datetime(linha).year, month=pd.to_datetime(linha).month, day=1)\n",
    "        periodo.append(data)\n",
    "\n",
    "    curva_forward['Periodo'] = periodo\n",
    "    curva_forward['Periodo'] = curva_forward['Periodo'].astype(\"datetime64\")\n",
    "\n",
    "    print('> Curva forward calculado com sucesso')\n",
    "\n",
    "    #Salva o Dataframe\n",
    "    _salva_dataframes(df=curva_forward, nome='curva_forward', formato='Excel')\n",
    "\n",
    "    # Retorna um Dataframe\n",
    "    return curva_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta a curva de precos realizados da Forecast Assumptions\n",
    "def precos_realizados_forecast_assumptions(mes_inicial= MES_INICIAL, forecast_assumption_diretorio= FORECAST_ASSUMPTIONS_DIR):\n",
    "    \n",
    "    # Dataframe do arquivo de Forecast Assumption com os PLDs realizados \n",
    "    forecast_assumptions_arq = pd.read_excel(forecast_assumption_diretorio, skiprows=1, sheet_name=\"Assumptions\")\n",
    "\n",
    "    # Checa se o primeiro mes do Book é igual ao primeiro mes do Forecast Assumptions\n",
    "    if forecast_assumptions_arq.columns[3] != pd.to_datetime(mes_inicial):\n",
    "        print('> Mes inicial da Forecast Assumptions diferente do Mes inicial do Book de Trading')\n",
    "        print('> Continuando sem realizado do PLD')\n",
    "        \n",
    "        forecast_assumptions = None\n",
    "    else:\n",
    "    # ETL do Forecast Assumptions\n",
    "        forecast_assumptions = forecast_assumptions_arq.iloc[:,1:15].loc[7:,:].reset_index(drop=True)\n",
    "        forecast_assumptions['FORECAST ASSUMPTIONS'] = forecast_assumptions.iloc[:,1]\n",
    "        forecast_assumptions['FORECAST ASSUMPTIONS'].replace({'Previous Forecast': pd.NaT , 'Forecast': pd.NaT }, inplace=True)\n",
    "        forecast_assumptions['FORECAST ASSUMPTIONS'].ffill(inplace=True)\n",
    "        forecast_assumptions.rename(columns={'Unnamed: 2': 'Tipo'}, inplace=True)\n",
    "        forecast_assumptions = forecast_assumptions[~forecast_assumptions.iloc[:,3].isna()]\n",
    "        forecast_assumptions = pd.melt(forecast_assumptions, id_vars=['FORECAST ASSUMPTIONS', 'Tipo'],var_name='Periodo', value_name=\"Valor\" )\n",
    "        print('> Precos realizados calculado com sucesso')\n",
    "\n",
    "    #Salva o Dataframe\n",
    "    _salva_dataframes(df=forecast_assumptions, nome='forecast_assumptions', formato='Excel')\n",
    "\n",
    "    # Retorna um Dataframe\n",
    "    return forecast_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monta a curva de precos que vai ser utilizada no Book de Trading\n",
    "def curva_precos_para_book(mes_inicial= MES_INICIAL):\n",
    "\n",
    "    # Roda a funcao da ultima curva forward\n",
    "    curva_forward =  ultima_curva_forward()\n",
    "\n",
    "    # Roda a funcao da dos precos realizados\n",
    "    forecast_assumptions =  precos_realizados_forecast_assumptions()\n",
    "\n",
    "    # Executa a funcao do periodo do book\n",
    "    periodo_book = _periodo_e_horas_meses()\n",
    "\n",
    "    # Define o mes inicial do Book\n",
    "    mes_inicial = pd.to_datetime(mes_inicial)\n",
    "\n",
    "    # Se o forecast_assumptions for None quer dizer que não tem PLD realizado ainda e \n",
    "    # Se eu não tiver o mes inicial na Curva Foward\n",
    "    if forecast_assumptions.empty and (mes_inicial in curva_forward['Periodo']) == False:\n",
    "        \n",
    "        print('> Não consigo montar os precos do Book pois não tenho os PLDS realizados nem a curva forward')\n",
    "        print('> Favor criar uma planilha igual ao Forecast Assumptions com a data correta e o PLD realizado')\n",
    "    \n",
    "    # Se somente o forecast_assumptions for None\n",
    "    else:\n",
    "        \n",
    "        # ETL da curva forward para o preco do Book\n",
    "        df_curva_forward = curva_forward[['dataSource', 'name', 'Periodo', 'vertexValue']]\n",
    "        df_curva_forward = df_curva_forward.rename(columns={'dataSource': 'Fonte', 'name': 'Tipo', 'vertexValue': 'Valor'})\n",
    "        df_curva_forward['Fonte'] = 'Ehub BBCCE'\n",
    "        df_curva_forward['Tipo'] = 'Forward'\n",
    "        df_curva_forward = df_curva_forward[df_curva_forward['Periodo'].isin(periodo_book['Periodo'])]\n",
    "\n",
    "        # Aumenta o periodo até coincidir com o periodo do Book\n",
    "        while not df_curva_forward['Periodo'].iloc[-1] == periodo_book['Periodo'].iloc[-1]:\n",
    "\n",
    "            ult_linha = df_curva_forward.iloc[-1].copy()\n",
    "            ult_linha['Periodo'] = pd.to_datetime(ult_linha['Periodo']) + pd.DateOffset(months=1)\n",
    "            df_curva_forward = df_curva_forward.append([ult_linha])\n",
    "        \n",
    "        precos_book = df_curva_forward.sort_values(by='Periodo').reset_index(drop=True)\n",
    "        \n",
    "        # Se somente o forecast_assumptions for None\n",
    "        if not forecast_assumptions.empty:\n",
    "\n",
    "            # ETL dos precos realizados para o preco do Book\n",
    "            filtro_1 = (forecast_assumptions['FORECAST ASSUMPTIONS'] == 'PLD SE')\n",
    "            filtro_2 = (forecast_assumptions['Tipo'] == \"Forecast\")\n",
    "            df_forecast_assumptions = forecast_assumptions.loc[filtro_1 & filtro_2].rename(columns={'FORECAST ASSUMPTIONS': 'Fonte' })\n",
    "            df_forecast_assumptions = df_forecast_assumptions[~df_forecast_assumptions['Periodo'].isin(df_curva_forward['Periodo'])]\n",
    "            df_forecast_assumptions['Fonte'] = 'Forecast Assumptions'\n",
    "            df_forecast_assumptions['Tipo'] = 'Realizado'\n",
    "\n",
    "            precos_book = pd.concat([df_curva_forward, df_forecast_assumptions]).sort_values(by='Periodo').reset_index(drop=True)\n",
    "        \n",
    "        print('> Precos para o Book montado com sucesso')\n",
    "\n",
    "    #Salva o Dataframe\n",
    "    _salva_dataframes(df=precos_book, nome='precos_book', formato='Excel')\n",
    "\n",
    "    # Retorna um Dataframe\n",
    "    return precos_book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reajuste Inflacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "infla_ipca = pd.read_pickle(f'{INFLACOES_DFS_DIR}/Inflacao_IPCA.df')\n",
    "infla_igpm = pd.read_pickle(f'{INFLACOES_DFS_DIR}/Inflacao_IGPM.df')\n",
    "bd_operacoes_modificada = pd.read_excel(f'{SALVA_OS_DFS}/bd_operacoes.xlsx')\n",
    "precos_book = pd.read_excel(f'{SALVA_OS_DFS}/precos_book.xlsx')\n",
    "periodos_e_horas = pd.read_excel(f'{SALVA_OS_DFS}/periodos_e_horas.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Timestamp('2020-06-01 00:00:00'), Timestamp('2020-07-01 00:00:00'),\n",
       "       Timestamp('2020-09-16 00:00:00'), Timestamp('2020-10-01 00:00:00'),\n",
       "       Timestamp('2020-12-01 00:00:00'), Timestamp('2020-12-21 00:00:00'),\n",
       "       Timestamp('2021-04-12 00:00:00'), Timestamp('2021-04-13 00:00:00'),\n",
       "       Timestamp('2021-04-14 00:00:00'), Timestamp('2021-05-14 00:00:00'),\n",
       "       Timestamp('2021-05-01 00:00:00'), Timestamp('2021-06-18 00:00:00'),\n",
       "       Timestamp('2021-06-01 00:00:00'), Timestamp('2021-07-01 00:00:00'),\n",
       "       Timestamp('2021-07-09 00:00:00'), Timestamp('2021-07-13 00:00:00'),\n",
       "       Timestamp('2021-08-01 00:00:00'), Timestamp('2021-07-29 00:00:00'),\n",
       "       Timestamp('2021-08-09 00:00:00'), Timestamp('2021-09-02 00:00:00'),\n",
       "       0, Timestamp('2021-09-21 00:00:00'),\n",
       "       Timestamp('2021-11-01 00:00:00'), Timestamp('2021-12-21 00:00:00'),\n",
       "       Timestamp('2021-12-23 00:00:00'), Timestamp('2021-12-28 00:00:00'),\n",
       "       Timestamp('2022-03-18 00:00:00'), Timestamp('2022-03-24 00:00:00'),\n",
       "       Timestamp('2022-04-01 00:00:00'), Timestamp('2022-04-07 00:00:00'),\n",
       "       Timestamp('2022-06-01 00:00:00'), Timestamp('2022-06-02 00:00:00'),\n",
       "       Timestamp('2022-06-03 00:00:00'), Timestamp('2022-08-04 00:00:00'),\n",
       "       Timestamp('2022-08-16 00:00:00'), Timestamp('2022-07-01 00:00:00'),\n",
       "       Timestamp('2022-08-17 00:00:00'), Timestamp('2022-08-19 00:00:00'),\n",
       "       Timestamp('2022-08-22 00:00:00'), Timestamp('2022-09-01 00:00:00'),\n",
       "       Timestamp('2022-08-25 00:00:00'), Timestamp('2022-09-05 00:00:00'),\n",
       "       Timestamp('2022-09-06 00:00:00'), Timestamp('2022-09-13 00:00:00'),\n",
       "       Timestamp('2022-09-15 00:00:00'), Timestamp('2022-09-19 00:00:00'),\n",
       "       Timestamp('2022-09-23 00:00:00'), Timestamp('2022-11-11 00:00:00'),\n",
       "       Timestamp('2022-11-01 00:00:00'), Timestamp('2022-12-06 00:00:00'),\n",
       "       Timestamp('2022-12-01 00:00:00'), Timestamp('2022-12-20 00:00:00')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_operacoes_modificada['DATA BASE'].fillna(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INICIO</th>\n",
       "      <th>FIM</th>\n",
       "      <th>VOL (MWm)</th>\n",
       "      <th>SPREAD</th>\n",
       "      <th>MOD PREÇO</th>\n",
       "      <th>PAGAMENTO</th>\n",
       "      <th>GARANTIAS</th>\n",
       "      <th>ÍNDICE</th>\n",
       "      <th>DATA BASE</th>\n",
       "      <th>VIGENTE</th>\n",
       "      <th>ESTIMADO</th>\n",
       "      <th>BBCE</th>\n",
       "      <th>SSJ</th>\n",
       "      <th>Enviada</th>\n",
       "      <th>Arquivada</th>\n",
       "      <th>Compra/venda</th>\n",
       "      <th>_</th>\n",
       "      <th>Periodo</th>\n",
       "      <th>Vol MWm</th>\n",
       "      <th>Preco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Fixo</td>\n",
       "      <td>6º du</td>\n",
       "      <td>Garantia Corporativa</td>\n",
       "      <td>IPCA</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SSJ 8489</td>\n",
       "      <td>Digital</td>\n",
       "      <td>2021-11-17 00:00:00</td>\n",
       "      <td>C</td>\n",
       "      <td>CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Fixo</td>\n",
       "      <td>6º du</td>\n",
       "      <td>Garantia Corporativa</td>\n",
       "      <td>IPCA</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SSJ 8489</td>\n",
       "      <td>Digital</td>\n",
       "      <td>2021-11-17 00:00:00</td>\n",
       "      <td>C</td>\n",
       "      <td>CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Fixo</td>\n",
       "      <td>6º du</td>\n",
       "      <td>Garantia Corporativa</td>\n",
       "      <td>IPCA</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SSJ 8489</td>\n",
       "      <td>Digital</td>\n",
       "      <td>2021-11-17 00:00:00</td>\n",
       "      <td>C</td>\n",
       "      <td>CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Fixo</td>\n",
       "      <td>6º du</td>\n",
       "      <td>Garantia Corporativa</td>\n",
       "      <td>IPCA</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SSJ 8489</td>\n",
       "      <td>Digital</td>\n",
       "      <td>2021-11-17 00:00:00</td>\n",
       "      <td>C</td>\n",
       "      <td>CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Fixo</td>\n",
       "      <td>6º du</td>\n",
       "      <td>Garantia Corporativa</td>\n",
       "      <td>IPCA</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SSJ 8489</td>\n",
       "      <td>Digital</td>\n",
       "      <td>2021-11-17 00:00:00</td>\n",
       "      <td>C</td>\n",
       "      <td>CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      INICIO        FIM  VOL (MWm)  SPREAD MOD PREÇO PAGAMENTO  \\\n",
       "0 2023-01-01 2023-12-31      18.75   150.0      Fixo     6º du   \n",
       "1 2023-01-01 2023-12-31      18.75   150.0      Fixo     6º du   \n",
       "2 2023-01-01 2023-12-31      18.75   150.0      Fixo     6º du   \n",
       "3 2023-01-01 2023-12-31      18.75   150.0      Fixo     6º du   \n",
       "4 2023-01-01 2023-12-31      18.75   150.0      Fixo     6º du   \n",
       "\n",
       "              GARANTIAS ÍNDICE  DATA BASE  VIGENTE  ESTIMADO BBCE       SSJ  \\\n",
       "0  Garantia Corporativa   IPCA 2020-06-01      NaN       NaN  NaN  SSJ 8489   \n",
       "1  Garantia Corporativa   IPCA 2020-06-01      NaN       NaN  NaN  SSJ 8489   \n",
       "2  Garantia Corporativa   IPCA 2020-06-01      NaN       NaN  NaN  SSJ 8489   \n",
       "3  Garantia Corporativa   IPCA 2020-06-01      NaN       NaN  NaN  SSJ 8489   \n",
       "4  Garantia Corporativa   IPCA 2020-06-01      NaN       NaN  NaN  SSJ 8489   \n",
       "\n",
       "   Enviada            Arquivada Compra/venda  \\\n",
       "0  Digital  2021-11-17 00:00:00            C   \n",
       "1  Digital  2021-11-17 00:00:00            C   \n",
       "2  Digital  2021-11-17 00:00:00            C   \n",
       "3  Digital  2021-11-17 00:00:00            C   \n",
       "4  Digital  2021-11-17 00:00:00            C   \n",
       "\n",
       "                                          _    Periodo  Vol MWm  Preco  \n",
       "0  CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO 2023-01-01    18.75  150.0  \n",
       "1  CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO 2023-02-01    18.75  150.0  \n",
       "2  CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO 2023-03-01    18.75  150.0  \n",
       "3  CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO 2023-04-01    18.75  150.0  \n",
       "4  CESP - COMPANHIA ENERGÉTICA DE SÃO PAULO 2023-05-01    18.75  150.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_operacoes_modificada.iloc[:5,12:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72ac7d4f0f8fd6315658ebf580d41236713753fa41d7edbc48193f480f78c166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
